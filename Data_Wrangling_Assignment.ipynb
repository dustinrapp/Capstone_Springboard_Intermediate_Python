{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project: Data Cleaning Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment: Create a short document (1-2 pages) in your github describing the data wrangling steps that you undertook to clean your capstone project data set. What kind of cleaning steps did you perform? How did you deal with missing values, if any? Were there outliers, and how did you decide to handle them? This document will eventually become part of your milestone report.**  \n",
    "  \n",
    "**In this notebook, raw data from the Copper Mountain, CO SNOTEL site and Copper Mountain, CO ASOS station will be wrangled for further use**\n",
    "\n",
    "------\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0: Import necessary modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime as dtb\n",
    "import os\n",
    "from glob import glob\n",
    "import datetime as dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.) Data Cleaning of Copper Mountain SNOTEL Data\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step A1: Hourly data files were downloaded on an annual basis.  Thus, there is one file for each year (2005-2017).  The data are comma separated and contain headers.  Each snotel data file row contains hourly or 3-hourly observations of the following parameters:**  \n",
    "**Field 1. Site_ID:**  NRCS Site Identifier  \n",
    "**Field 2. Date:**  Date of observation  \n",
    "**Field 3. Time:** Hour of observation  \n",
    "**Field 4. WTEQ.I-1 (in):** Recorded Water Equivalent of snow  \n",
    "**Field 5. PREC.I-1 (in):** Recorded Precipitation  \n",
    "**Field 6. TOBS.I-1 (DegC):** Recorded Temperature  \n",
    "**Field 7. SNWD.I (in):** Recorded Snow Depth  Here, the data is read in using the glob funcction**  \n",
    "**Here, the each data file is read in.  The date and time columns are merged together and made into the index for the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\SNOTEL\\\\415_STAND_YEAR=2005.csv', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\SNOTEL\\\\415_STAND_YEAR=2006.csv', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\SNOTEL\\\\415_STAND_YEAR=2007.csv', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\SNOTEL\\\\415_STAND_YEAR=2008.csv', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\SNOTEL\\\\415_STAND_YEAR=2009.csv', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\SNOTEL\\\\415_STAND_YEAR=2010.csv', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\SNOTEL\\\\415_STAND_YEAR=2011.csv', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\SNOTEL\\\\415_STAND_YEAR=2012.csv', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\SNOTEL\\\\415_STAND_YEAR=2013.csv', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\SNOTEL\\\\415_STAND_YEAR=2014.csv', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\SNOTEL\\\\415_STAND_YEAR=2015.csv', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\SNOTEL\\\\415_STAND_YEAR=2016.csv', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\SNOTEL\\\\415_STAND_YEAR=2017.csv', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\SNOTEL\\\\415_STAND_YEAR=2018.csv']\n",
      "                     Site Id  WTEQ.I-1 (in)   PREC.I-1 (in)   \\\n",
      "Date_Time                                                      \n",
      "2005-01-01 00:00:00      415             5.2             5.8   \n",
      "2005-01-01 03:00:00      415             5.2             6.0   \n",
      "2005-01-02 00:00:00      415             5.1             5.8   \n",
      "2005-01-02 03:00:00      415             5.1             5.8   \n",
      "2005-01-03 00:00:00      415             5.1             5.9   \n",
      "\n",
      "                     TOBS.I-1 (degC)   SNWD.I-1 (in)   Unnamed: 7  \n",
      "Date_Time                                                          \n",
      "2005-01-01 00:00:00              -2.1           -99.9         NaN  \n",
      "2005-01-01 03:00:00              -4.2           -99.9         NaN  \n",
      "2005-01-02 00:00:00             -11.2           -99.9         NaN  \n",
      "2005-01-02 03:00:00             -11.9           -99.9         NaN  \n",
      "2005-01-03 00:00:00              -8.9           -99.9         NaN  \n",
      "        Site Id  WTEQ.I-1 (in)   PREC.I-1 (in)   TOBS.I-1 (degC)   \\\n",
      "count  104069.0   104069.000000   104069.000000     104069.000000   \n",
      "mean      415.0        0.519156       14.761584          1.323900   \n",
      "std         0.0       24.188216       15.142395          9.345465   \n",
      "min       415.0      -99.900000      -99.900000        -99.900000   \n",
      "25%       415.0        0.000000        7.000000         -5.000000   \n",
      "50%       415.0        2.600000       16.200000          1.300000   \n",
      "75%       415.0       10.700000       23.600000          7.700000   \n",
      "max       415.0       26.200000       37.900000         25.800000   \n",
      "\n",
      "       SNWD.I-1 (in)   Unnamed: 7  \n",
      "count   104069.000000         0.0  \n",
      "mean         6.345225         NaN  \n",
      "std         44.534695         NaN  \n",
      "min       -196.000000         NaN  \n",
      "25%          0.000000         NaN  \n",
      "50%         10.000000         NaN  \n",
      "75%         37.000000         NaN  \n",
      "max        189.000000         NaN  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "snotel_files = glob(r'C:\\Users\\RAPP\\Documents\\Capstone\\data\\SNOTEL\\415_STAND_YEAR=*.csv')\n",
    "print(snotel_files)\n",
    "snotel_data = [pd.read_csv(f, header=1, parse_dates=[['Date', 'Time']], index_col='Date_Time') for f in snotel_files]\n",
    "\n",
    "snotel_df= pd.concat(snotel_data)\n",
    "print(snotel_df.head())\n",
    "print(snotel_df.describe())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step A2:  The missing code is for all data is --99.  Set these to Nan values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Site Id  WTEQ.I-1 (in)   PREC.I-1 (in)   TOBS.I-1 (degC)   \\\n",
      "count  104069.0    98736.000000   102990.000000     104027.000000   \n",
      "mean      415.0        5.943067       15.962864          1.364769   \n",
      "std         0.0        6.525494        9.618173          9.123291   \n",
      "min       415.0      -12.000000       -0.300000        -33.100000   \n",
      "25%       415.0        0.000000        7.200000         -5.000000   \n",
      "50%       415.0        3.500000       16.300000          1.300000   \n",
      "75%       415.0       11.100000       23.700000          7.700000   \n",
      "max       415.0       26.200000       37.900000         25.800000   \n",
      "\n",
      "       SNWD.I-1 (in)   Unnamed: 7  \n",
      "count    91277.000000         0.0  \n",
      "mean        21.234944         NaN  \n",
      "std         21.391861         NaN  \n",
      "min       -196.000000         NaN  \n",
      "25%          0.000000         NaN  \n",
      "50%         18.000000         NaN  \n",
      "75%         39.000000         NaN  \n",
      "max        189.000000         NaN  \n"
     ]
    }
   ],
   "source": [
    "#set outliers and missing value to Nan\n",
    "xx=(snotel_df[:]==-99.9)\n",
    "snotel_df[xx]=np.NaN\n",
    "print(snotel_df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step A3:  The dataset should have a value for every hour, so use asfreq to make sure there is an observation for each hour.  The fill_value will be set to NaN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Site Id  WTEQ.I-1 (in)   PREC.I-1 (in)   TOBS.I-1 (degC)   \\\n",
      "count  104045.0    98725.000000   102968.000000     104027.000000   \n",
      "mean      415.0        5.943730       15.960916          1.364769   \n",
      "std         0.0        6.525556        9.617842          9.123291   \n",
      "min       415.0      -12.000000       -0.300000        -33.100000   \n",
      "25%       415.0        0.000000        7.200000         -5.000000   \n",
      "50%       415.0        3.500000       16.300000          1.300000   \n",
      "75%       415.0       11.100000       23.700000          7.700000   \n",
      "max       415.0       26.200000       37.900000         25.800000   \n",
      "\n",
      "       SNWD.I-1 (in)   Unnamed: 7  \n",
      "count    91266.000000         0.0  \n",
      "mean        21.237504         NaN  \n",
      "std         21.391880         NaN  \n",
      "min       -196.000000         NaN  \n",
      "25%          0.000000         NaN  \n",
      "50%         18.000000         NaN  \n",
      "75%         39.000000         NaN  \n",
      "max        189.000000         NaN  \n"
     ]
    }
   ],
   "source": [
    "snotel_df=snotel_df.asfreq(freq='1H', fill_value=np.NaN)\n",
    "print(snotel_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step A4:  As temperature and snow depth are the only variables of future interest, save the snow depth and temperature columns of this dataframe as a tab delimted file, ready for further analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "snotel_df.to_csv('snotel_df.dat',sep = ',', float_format = '%.2f',columns=['TOBS.I-1 (degC) ', 'SNWD.I-1 (in) '])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.) Data Cleaning of Copper Mountain, CO ASOS Data \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step B1: The raw ASOS data was obtained in Integrated Surface Hourly Lite format. A single file with hourly values was downloaded for each year (2006-2017).  According to documentation, this format is fixed format delimtited by whitespace.  From the data documentation, the data has 12 columns:  **\n",
    "\n",
    "**Field 1: Pos 1-4, Length 4: Observation Year  \n",
    "Field 2: Pos 6-7, Length 2: Observation Month  \n",
    "Field 3: Pos 9-11, Length 2: Observation Day  \n",
    "Field 4: Pos 12-13, Length 2: Observation Hour  \n",
    "Field 5: Pos 14-19, Length 6:  Air Temperature, Units: deg C scaled by factor of 10  \n",
    "Field 6: Pos 20-24, Length 6: Dew Point Temperature, Units: deg C scaled by factor of 10  \n",
    "Field 7: Pos 26-31, Length 6: Sea Level Pressure, Units: hectoPascals scaled by factor of 10  \n",
    "Field 8: Pos 32-37, Length 6: Wind Direction, Units: angular degrees  \n",
    "Field 9: Pos 38-43, Length 6: Wind Speed Rate, Units: m/s scaled by factor of 10  \n",
    "Field 10: Pos 44-49, Length 6: Sky Condition Total Coverage Code  \n",
    "Field 11: Pos 50-55, Length 6: Liquid Precipitation Depth Dimension - One Hour Duration, Units: mm scaled by factor of 10  \n",
    "Field 12: Pos 56-61, Length 6: Liquid Precipitation Depth Dimension - Six Hour Duration, Units: mm scaled by factor of 10  **\n",
    "  \n",
    "**Below the data files are read in using pd.read_csv. Specific header names appropriate for each column are specified in the header_names list. In addition, the dataframe index is created using the first four date/time columns.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038-2006\\\\722061-03038-2006', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038-2007\\\\722061-03038-2007', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038-2008\\\\722061-03038-2008', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038-2009\\\\722061-03038-2009', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038-2010\\\\722061-03038-2010', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038-2011\\\\722061-03038-2011', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038-2012\\\\722061-03038-2012', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038-2013\\\\722061-03038-2013', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038-2014\\\\722061-03038-2014', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038-2015\\\\722061-03038-2015', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038-2017\\\\722061-03038-2017']\n",
      "                     Temperature  Dewpoint  Pressure  WindDirection  \\\n",
      "Date_Time                                                             \n",
      "2006-01-01 01:00:00          -10       -70     -9999            220   \n",
      "2006-01-01 02:00:00          -30       -80     -9999            230   \n",
      "2006-01-01 03:00:00          -40       -80     -9999            230   \n",
      "2006-01-01 04:00:00          -40       -90     -9999            240   \n",
      "2006-01-01 05:00:00          -50       -80     -9999            210   \n",
      "\n",
      "                     WindSpeed  CloudCover  1hr_Precipitation  \\\n",
      "Date_Time                                                       \n",
      "2006-01-01 01:00:00         77       -9999              -9999   \n",
      "2006-01-01 02:00:00         51       -9999              -9999   \n",
      "2006-01-01 03:00:00         36       -9999              -9999   \n",
      "2006-01-01 04:00:00         62       -9999              -9999   \n",
      "2006-01-01 05:00:00         62       -9999              -9999   \n",
      "\n",
      "                     6hr_Precipitation  \n",
      "Date_Time                               \n",
      "2006-01-01 01:00:00              -9999  \n",
      "2006-01-01 02:00:00              -9999  \n",
      "2006-01-01 03:00:00              -9999  \n",
      "2006-01-01 04:00:00              -9999  \n",
      "2006-01-01 05:00:00              -9999  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "asos_files = glob(r'C:\\Users\\RAPP\\Documents\\Capstone\\data\\ASOS\\722061-03038*\\722061-03038*')\n",
    "print(asos_files)\n",
    "\n",
    "header_names = ('Year', 'Month', 'Day', 'Hour', 'Temperature', 'Dewpoint', 'Pressure', 'WindDirection', 'WindSpeed', 'CloudCover', '1hr_Precipitation', '6hr_Precipitation')\n",
    "#asos_data = [pd.read_csv(f, delim_whitespace=True, header = None) for f in asos_files]\n",
    "asos_data = [pd.read_csv(f, delim_whitespace=True, header = None, names = header_names, parse_dates={'Date_Time': ['Year', 'Month', 'Day', 'Hour']}, index_col='Date_Time') for f in asos_files]\n",
    "#parse_dates={'Date_Time': ['Year', 'Month', 'Day', 'Hour']\n",
    "raw_asos_df= pd.concat(asos_data)\n",
    "\n",
    "print(raw_asos_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step B2. Make a copy of the raw dataframe and call it asos_df.  Most of the raw data is in non standard units (e.g. deg C/10) will be loaded and scaled to  more standard units.  Thus column name in the raw dataframe are updated to reflect this.  All missing data filling will be performed in new dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Temperature', 'Dewpoint', 'Pressure', 'WindDirection', 'WindSpeed',\n",
      "       'CloudCover', '1hr_Precipitation', '6hr_Precipitation'],\n",
      "      dtype='object')\n",
      "DatetimeIndex(['2006-01-01 01:00:00', '2006-01-01 02:00:00',\n",
      "               '2006-01-01 03:00:00', '2006-01-01 04:00:00',\n",
      "               '2006-01-01 05:00:00', '2006-01-01 06:00:00',\n",
      "               '2006-01-01 07:00:00', '2006-01-01 08:00:00',\n",
      "               '2006-01-01 09:00:00', '2006-01-01 10:00:00',\n",
      "               ...\n",
      "               '2017-12-04 13:00:00', '2017-12-04 14:00:00',\n",
      "               '2017-12-04 15:00:00', '2017-12-04 17:00:00',\n",
      "               '2017-12-04 18:00:00', '2017-12-04 19:00:00',\n",
      "               '2017-12-04 20:00:00', '2017-12-04 21:00:00',\n",
      "               '2017-12-04 22:00:00', '2017-12-04 23:00:00'],\n",
      "              dtype='datetime64[ns]', name='Date_Time', length=76125, freq=None)\n",
      "Index(['Temperature_degC', 'Dewpoint_degC', 'Pressure_hp', 'WindDirection_deg',\n",
      "       'WindSpeed_m/s', 'CloudCover', '1hr_Precipitation_mm',\n",
      "       '6hr_Precipitation'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "asos_df = raw_asos_df[:].copy()\n",
    "print(asos_df.keys())\n",
    "print(asos_df.index)\n",
    "asos_df.rename(columns={'Temperature': 'Temperature_degC', 'Dewpoint': 'Dewpoint_degC', 'Pressure': 'Pressure_hp' , 'WindSpeed': 'WindSpeed_m/s', 'WindDirection': 'WindDirection_deg', '1hr_Precipitation': '1hr_Precipitation_mm', '6hr_Precipitation_mm': 'PREC_6hr_mm'}, inplace=True)\n",
    "print(asos_df.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step B3.  The data documentation states that all missing values are set to -9999.  These will be changed to np.NaN in the asos_df dataframe. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Temperature      Dewpoint  Pressure  WindDirection     WindSpeed  \\\n",
      "count  76125.000000  76125.000000   76125.0   76125.000000  76125.000000   \n",
      "mean      13.537800   -107.901176   -9999.0    -502.719724   -659.407094   \n",
      "std      168.622366    436.731206       0.0    2638.266234   2594.026616   \n",
      "min    -9999.000000  -9999.000000   -9999.0   -9999.000000  -9999.000000   \n",
      "25%      -50.000000   -140.000000   -9999.0     180.000000     31.000000   \n",
      "50%       10.000000    -90.000000   -9999.0     250.000000     57.000000   \n",
      "75%       90.000000    -30.000000   -9999.0     270.000000     82.000000   \n",
      "max      240.000000    100.000000   -9999.0     360.000000    283.000000   \n",
      "\n",
      "         CloudCover  1hr_Precipitation  6hr_Precipitation  \n",
      "count  76125.000000       76125.000000            76125.0  \n",
      "mean   -1951.280000       -9859.765163            -9999.0  \n",
      "std     3965.987069        1172.262188                0.0  \n",
      "min    -9999.000000       -9999.000000            -9999.0  \n",
      "25%        0.000000       -9999.000000            -9999.0  \n",
      "50%        0.000000       -9999.000000            -9999.0  \n",
      "75%        7.000000       -9999.000000            -9999.0  \n",
      "max        9.000000         140.000000            -9999.0  \n",
      "       Temperature_degC  Dewpoint_degC  Pressure_hp  WindDirection_deg  \\\n",
      "count      76110.000000   75982.000000          0.0       70674.000000   \n",
      "mean          15.511102     -89.285883          NaN         229.716869   \n",
      "std           93.151409      81.353954          NaN          72.902342   \n",
      "min         -280.000000    -450.000000          NaN           0.000000   \n",
      "25%          -50.000000    -140.000000          NaN         190.000000   \n",
      "50%           10.000000     -90.000000          NaN         260.000000   \n",
      "75%           90.000000     -30.000000          NaN         280.000000   \n",
      "max          240.000000     100.000000          NaN         360.000000   \n",
      "\n",
      "       WindSpeed_m/s    CloudCover  1hr_Precipitation_mm  6hr_Precipitation  \n",
      "count   70674.000000  61250.000000           1059.000000                0.0  \n",
      "mean       60.944393      3.166286              9.736544                NaN  \n",
      "std        34.360041      3.585964             12.352705                NaN  \n",
      "min         0.000000      0.000000              2.000000                NaN  \n",
      "25%        36.000000      0.000000              3.000000                NaN  \n",
      "50%        57.000000      0.000000              5.000000                NaN  \n",
      "75%        82.000000      8.000000             10.000000                NaN  \n",
      "max       283.000000      9.000000            140.000000                NaN   None\n"
     ]
    }
   ],
   "source": [
    "#set outliers and missing value to Nan\n",
    "xx= asos_df[:] == -9999.0\n",
    "asos_df[xx]=np.NaN\n",
    "\n",
    "\n",
    "print(asos_df.describe(), print(raw_asos_df.describe()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step B4. The data should contain an observation for each hour. The function asfreq will be to insure that is the case.  Any missing hours will be filled with np.Nan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Temperature_degC  Dewpoint_degC  Pressure_hp  WindDirection_deg  \\\n",
      "count      76110.000000   75982.000000          0.0       70674.000000   \n",
      "mean          15.511102     -89.285883          NaN         229.716869   \n",
      "std           93.151409      81.353954          NaN          72.902342   \n",
      "min         -280.000000    -450.000000          NaN           0.000000   \n",
      "25%          -50.000000    -140.000000          NaN         190.000000   \n",
      "50%           10.000000     -90.000000          NaN         260.000000   \n",
      "75%           90.000000     -30.000000          NaN         280.000000   \n",
      "max          240.000000     100.000000          NaN         360.000000   \n",
      "\n",
      "       WindSpeed_m/s    CloudCover  1hr_Precipitation_mm  6hr_Precipitation  \n",
      "count   70674.000000  61250.000000           1059.000000                0.0  \n",
      "mean       60.944393      3.166286              9.736544                NaN  \n",
      "std        34.360041      3.585964             12.352705                NaN  \n",
      "min         0.000000      0.000000              2.000000                NaN  \n",
      "25%        36.000000      0.000000              3.000000                NaN  \n",
      "50%        57.000000      0.000000              5.000000                NaN  \n",
      "75%        82.000000      8.000000             10.000000                NaN  \n",
      "max       283.000000      9.000000            140.000000                NaN  \n"
     ]
    }
   ],
   "source": [
    "asos_df=asos_df.asfreq(freq='1H', fill_value=np.NaN)\n",
    "print(asos_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step B5: According to documentation, many of the variables' data have been scaled by a factor of 10.  To get actual values in more standard units for some variables, the raw data needs to be divided by 10.  \n",
    "This operation will be saved in the copied dataframe just made.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Temperature_degC  Dewpoint_degC  Pressure_hp  WindDirection_deg  \\\n",
      "count      76125.000000   75982.000000          0.0       70674.000000   \n",
      "mean           1.353780     -89.285883          NaN         229.716869   \n",
      "std           16.862237      81.353954          NaN          72.902342   \n",
      "min         -999.900000    -450.000000          NaN           0.000000   \n",
      "25%           -5.000000    -140.000000          NaN         190.000000   \n",
      "50%            1.000000     -90.000000          NaN         260.000000   \n",
      "75%            9.000000     -30.000000          NaN         280.000000   \n",
      "max           24.000000     100.000000          NaN         360.000000   \n",
      "\n",
      "       WindSpeed_m/s    CloudCover  1hr_Precipitation_mm  6hr_Precipitation  \\\n",
      "count   70674.000000  61250.000000           1059.000000                0.0   \n",
      "mean       60.944393      3.166286              9.736544                NaN   \n",
      "std        34.360041      3.585964             12.352705                NaN   \n",
      "min         0.000000      0.000000              2.000000                NaN   \n",
      "25%        36.000000      0.000000              3.000000                NaN   \n",
      "50%        57.000000      0.000000              5.000000                NaN   \n",
      "75%        82.000000      8.000000             10.000000                NaN   \n",
      "max       283.000000      9.000000            140.000000                NaN   \n",
      "\n",
      "            DP_degC        SLP_hp        WS_m/s   PREC_1hr_mm   PREC_6hr_mm  \n",
      "count  76125.000000  7.612500e+04  76125.000000  76125.000000  7.612500e+04  \n",
      "mean     -10.790118 -9.999000e+02    -65.940709   -985.976516 -9.999000e+02  \n",
      "std       43.673121  3.410628e-13    259.402662    117.226219  3.410628e-13  \n",
      "min     -999.900000 -9.999000e+02   -999.900000   -999.900000 -9.999000e+02  \n",
      "25%      -14.000000 -9.999000e+02      3.100000   -999.900000 -9.999000e+02  \n",
      "50%       -9.000000 -9.999000e+02      5.700000   -999.900000 -9.999000e+02  \n",
      "75%       -3.000000 -9.999000e+02      8.200000   -999.900000 -9.999000e+02  \n",
      "max       10.000000 -9.999000e+02     28.300000     14.000000 -9.999000e+02  \n"
     ]
    }
   ],
   "source": [
    "asos_df['Temperature_degC'] = raw_asos_df['Temperature']/10\n",
    "asos_df['DP_degC'] = raw_asos_df['Dewpoint']/10\n",
    "asos_df['SLP_hp'] = raw_asos_df['Pressure']/10\n",
    "asos_df['WS_m/s'] = raw_asos_df['WindSpeed']/10\n",
    "asos_df['PREC_1hr_mm'] = raw_asos_df['1hr_Precipitation']/10\n",
    "asos_df['PREC_6hr_mm'] = raw_asos_df['6hr_Precipitation']/10\n",
    "\n",
    "\n",
    "print(asos_df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step B6. Now write the final dataframe to a tab delimited csv file for future use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "asos_df.to_csv('asos_df.dat',sep = ',', float_format = '%.2f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
