{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface Data Preparation (part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  \n",
    "## **In this notebook, raw data from the Copper Mountain, CO SNOTEL site and Copper Mountain, CO AWOS station is organized, combined and and preprocessed into csv files for future use **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0: Import necessary modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime as dtb\n",
    "import os\n",
    "from glob import glob\n",
    "import datetime as dt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')   #suppress warning messages for cleaner presentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.) Data Cleaning of Copper Mountain SNOTEL Data\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step A1: Hourly data files were downloaded on an annual basis.  Thus, there is one file for each year (2005-2017).  The data are comma separated and contain headers.  Each snotel data file row contains hourly or 3-hourly observations of the following parameters:**  \n",
    "**Field 1. Site_ID:**  NRCS Site Identifier  \n",
    "**Field 2. Date:**  Date of observation  \n",
    "**Field 3. Time:** Hour of observation  \n",
    "**Field 4. WTEQ.I-1 (in):** Recorded Water Equivalent of snow  \n",
    "**Field 5. PREC.I-1 (in):** Recorded Precipitation  \n",
    "**Field 6. TOBS.I-1 (DegC):** Recorded Temperature  \n",
    "**Field 7. SNWD.I (in):** Recorded Snow Depth  Here, the data is read in using the glob funcction**  \n",
    "**Here, the each data file is read in.  The date and time columns are merged together and made into the index for the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "snotel_files = glob(r'C:\\Users\\RAPP\\Documents\\Capstone\\data\\SNOTEL\\415_STAND_YEAR=*.csv')\n",
    "#print(snotel_files)\n",
    "snotel_data = [pd.read_csv(f, header=1, parse_dates=[['Date', 'Time']], index_col='Date_Time') for f in snotel_files]\n",
    "\n",
    "snotel_df= pd.concat(snotel_data)\n",
    "#print(snotel_df.head())\n",
    "#print(snotel_df.describe())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step A2:  The missing code is for all data is --99.  Set these to Nan values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set outliers and missing value to Nan\n",
    "xx=(snotel_df[:]==-99.9)\n",
    "snotel_df[xx]=np.NaN\n",
    "\n",
    "#print(snotel_df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step A3:  The dataset should have a value for every hour, so use asfreq to make sure there is an observation for each hour.  The fill_value will be set to NaN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "snotel_df=snotel_df.asfreq(freq='1H', fill_value=np.NaN)\n",
    "#print(snotel_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step A4:  As temperature and snow depth are the only variables of future interest, save the snow depth and temperature columns of this dataframe as a tab delimted file, ready for further analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "snotel_df.to_csv('snotel_df.dat',sep = ',', float_format = '%.2f',columns=['TOBS.I-1 (degC) ', 'SNWD.I-1 (in) '])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.) Data Cleaning of Copper Mountain, CO ASOS Data \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step B1: The raw ASOS data was obtained in Integrated Surface Hourly Lite format. A single file with hourly values was downloaded for each year (2006-2017).  According to documentation, this format is fixed format delimtited by whitespace.  From the data documentation, the data has 12 columns:  **\n",
    "\n",
    "**Field 1: Pos 1-4, Length 4: Observation Year  \n",
    "Field 2: Pos 6-7, Length 2: Observation Month  \n",
    "Field 3: Pos 9-11, Length 2: Observation Day  \n",
    "Field 4: Pos 12-13, Length 2: Observation Hour  \n",
    "Field 5: Pos 14-19, Length 6:  Air Temperature, Units: deg C scaled by factor of 10  \n",
    "Field 6: Pos 20-24, Length 6: Dew Point Temperature, Units: deg C scaled by factor of 10  \n",
    "Field 7: Pos 26-31, Length 6: Sea Level Pressure, Units: hectoPascals scaled by factor of 10  \n",
    "Field 8: Pos 32-37, Length 6: Wind Direction, Units: angular degrees  \n",
    "Field 9: Pos 38-43, Length 6: Wind Speed Rate, Units: m/s scaled by factor of 10  \n",
    "Field 10: Pos 44-49, Length 6: Sky Condition Total Coverage Code  \n",
    "Field 11: Pos 50-55, Length 6: Liquid Precipitation Depth Dimension - One Hour Duration, Units: mm scaled by factor of 10  \n",
    "Field 12: Pos 56-61, Length 6: Liquid Precipitation Depth Dimension - Six Hour Duration, Units: mm scaled by factor of 10  **\n",
    "  \n",
    "**Below the data files are read in using pd.read_csv. Specific header names appropriate for each column are specified in the header_names list. In addition, the dataframe index is created using the first four date/time columns.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038\\\\722061-03038-2006\\\\722061-03038-2006', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038\\\\722061-03038-2007\\\\722061-03038-2007', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038\\\\722061-03038-2008\\\\722061-03038-2008', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038\\\\722061-03038-2009\\\\722061-03038-2009', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038\\\\722061-03038-2010\\\\722061-03038-2010', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038\\\\722061-03038-2011\\\\722061-03038-2011', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038\\\\722061-03038-2012\\\\722061-03038-2012', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038\\\\722061-03038-2013\\\\722061-03038-2013', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038\\\\722061-03038-2014\\\\722061-03038-2014', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038\\\\722061-03038-2015\\\\722061-03038-2015', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038\\\\722061-03038-2016\\\\722061-03038-2016', 'C:\\\\Users\\\\RAPP\\\\Documents\\\\Capstone\\\\data\\\\ASOS\\\\722061-03038\\\\722061-03038-2017\\\\722061-03038-2017']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "awos_files = glob(r'C:\\Users\\RAPP\\Documents\\Capstone\\data\\ASOS\\722061-03038\\722061-03038*\\722061-03038*')\n",
    "print(awos_files)\n",
    "\n",
    "header_names = ('Year', 'Month', 'Day', 'Hour', 'Temperature', 'Dewpoint', 'Pressure', 'WindDirection', 'WindSpeed', 'CloudCover', '1hr_Precipitation', '6hr_Precipitation')\n",
    "\n",
    "awos_data = [pd.read_csv(f, delim_whitespace=True, header = None, names = header_names, parse_dates={'Date_Time': ['Year', 'Month', 'Day', 'Hour']}, index_col='Date_Time') for f in awos_files]\n",
    "#parse_dates={'Date_Time': ['Year', 'Month', 'Day', 'Hour']\n",
    "raw_awos_df= pd.concat(awos_data)\n",
    "\n",
    "#print(raw_awos_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step B2. Make a copy of the raw dataframe and call it asos_df.  Most of the raw data is in non standard units (e.g. deg C/10) will be loaded and scaled to  more standard units.  Thus column name in the raw dataframe are updated to reflect this.  All missing data filling will be performed in new dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "awos_df = raw_awos_df[:].copy()\n",
    "#print(awos_df.keys())\n",
    "#print(awos_df.index)\n",
    "awos_df.rename(columns={'Temperature': 'Temperature_degC', 'Dewpoint': 'Dewpoint_degC', 'Pressure': 'Pressure_hp' , 'WindSpeed': 'WindSpeed_m/s', 'WindDirection': 'WindDirection_deg', '1hr_Precipitation': '1hr_Precipitation_mm', '6hr_Precipitation': '6hr_Precipitation_mm'}, inplace=True)\n",
    "#print(awos_df.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step B3. The data should contain an observation for each hour. The function asfreq will be to insure that is the case.  Any missing hours will be filled with np.Nan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "awos_df=awos_df.asfreq(freq='1H', fill_value=np.NaN)\n",
    "#print(asos_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step B4.  The data documentation states that all missing values are set to -9999.  These will be changed to np.NaN in the asos_df dataframe. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set outliers and missing value to Nan\n",
    "xx= awos_df[:] == -9999.0\n",
    "awos_df[xx]=np.NaN\n",
    "\n",
    "\n",
    "#print(awos_df.describe())\n",
    "#print(raw_awos_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step B5: According to documentation, many of the variables' data have been scaled by a factor of 10.  To get actual values in more standard units for some variables, the raw data needs to be divided by 10.  \n",
    "This operation will be saved in the copied dataframe just made.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Temperature_degC  Dewpoint_degC  Pressure_hp  WindDirection_deg  \\\n",
      "count      82634.000000   82491.000000          0.0       76327.000000   \n",
      "mean           1.610124      -8.923046          NaN         230.179360   \n",
      "std            9.293641       8.115253          NaN          73.606778   \n",
      "min          -28.000000     -45.000000          NaN           0.000000   \n",
      "25%           -5.000000     -14.000000          NaN         190.000000   \n",
      "50%            1.000000      -9.000000          NaN         260.000000   \n",
      "75%            9.000000      -3.000000          NaN         280.000000   \n",
      "max           24.000000      10.000000          NaN         360.000000   \n",
      "\n",
      "       WindSpeed_m/s    CloudCover  1hr_Precipitation_mm  6hr_Precipitation_mm  \n",
      "count   76327.000000  65100.000000           1152.000000                   0.0  \n",
      "mean        6.047255      3.023717              0.955903                   NaN  \n",
      "std         3.430585      3.548211              1.232150                   NaN  \n",
      "min         0.000000      0.000000              0.200000                   NaN  \n",
      "25%         3.600000      0.000000              0.300000                   NaN  \n",
      "50%         5.700000      0.000000              0.500000                   NaN  \n",
      "75%         8.200000      8.000000              1.000000                   NaN  \n",
      "max        28.300000      9.000000             14.000000                   NaN  \n"
     ]
    }
   ],
   "source": [
    "awos_df['Temperature_degC'] = awos_df['Temperature_degC']/10\n",
    "awos_df['Dewpoint_degC'] = awos_df['Dewpoint_degC']/10\n",
    "awos_df['Pressure_hp'] = awos_df['Pressure_hp']/10\n",
    "awos_df['WindSpeed_m/s'] = awos_df['WindSpeed_m/s']/10\n",
    "awos_df['1hr_Precipitation_mm'] = awos_df['1hr_Precipitation_mm']/10\n",
    "awos_df['6hr_Precipitation_mm'] = awos_df['6hr_Precipitation_mm']/10\n",
    "\n",
    "\n",
    "print(awos_df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step B6. Now write the final dataframe to a tab delimited csv file for future use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "awos_df.to_csv('awos_df.dat',sep = ',', float_format = '%.2f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
